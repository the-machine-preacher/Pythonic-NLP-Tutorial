{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ad4785",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 1. Goals of this Unit\n",
    "*Natural Language Processing*\n",
    "\n",
    "----\n",
    "The goal of this unit is to introduce the field of natural language processing and provide an overview of common applications, techniques, and challenges.\n",
    "\n",
    "<br/>After this unit, you will be able to:\n",
    "- Understand what natural language processing is.\n",
    "- Gain an introduction to common applications and challenges within natural language processing.\n",
    "- Identify several natural language processing techniques and how they relate to each other.\n",
    "- Try out a few natural language processing techniques using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d84634",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 2. Intro to NLP\n",
    "*Natural Language Processing*\n",
    "\n",
    "----\n",
    "Look at the technologies around us:\n",
    "- Spellcheck and autocorrect\n",
    "- Auto-generated video captions\n",
    "- Virtual assistants like Amazon’s Alexa\n",
    "- Autocomplete\n",
    "- Your news site’s suggested articles\n",
    "\n",
    "<br/>What do they have in common?\n",
    "\n",
    "<br/>All of these handy technologies exist because of *natural language processing!* Also known as NLP, the field is at the intersection of linguistics, artificial intelligence, and computer science. The goal? Enabling computers to interpret, analyze, and approximate the generation of human languages (like English or Spanish).\n",
    "\n",
    "<br/>NLP got its start around 1950 with Alan Turing’s test for artificial intelligence evaluating whether a computer can use language to fool humans into believing it’s human.\n",
    "\n",
    "<br/>But approximating human speech is only one of a wide range of applications for NLP! Applications from detecting spam emails or bias in tweets to improving accessibility for people with disabilities all rely heavily on natural language processing techniques.\n",
    "\n",
    "<br/>NLP can be conducted in several programming languages. However, Python has some of the most extensive open-source NLP libraries, including the Natural Language Toolkit or *NLTK.* Because of this, you’ll be using Python to get your first taste of NLP.\n",
    "<img src=\"Images/Natural_Language_Processing_Overview.webp\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728bc94",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 3. Text Preprocessing\n",
    "*Natural Language Processing*\n",
    "\n",
    "----\n",
    "> \"You never know what you have... until you clean your data.\"\n",
    "\n",
    "Cleaning and preparation are crucial for many tasks, and NLP is no exception. *Text preprocessing* is usually the first step you’ll take when faced with an NLP task.\n",
    "\n",
    "<br/>Without preprocessing, your computer interprets `\"the\"`, `\"The\"`, and `\"<p>The\"` as entirely different words. There is a LOT you can do here, depending on the formatting you need. Lucky for you, Regex and NLTK will do most of it for you! Common tasks include:\n",
    "\n",
    "<br/>**Noise removal** — stripping text of formatting (e.g., HTML tags).\n",
    "\n",
    "<br/>**Tokenization** — breaking text into individual words.\n",
    "\n",
    "<br/>**Normalization** — cleaning text data in any other way:\n",
    "- *Stemming* is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “computer” may become “comput” and “are” would remain “are.”\n",
    "- *Lemmatization* is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "- Other common tasks include lowercasing, stopwords removal, spelling correction, etc.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. We used NLTK’s PorterStemmer to normalize the text — run the code to see how it does. (It may take a few seconds for the code to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498a9f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['so', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'i', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'i', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "#from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = None\n",
    "lemmatized = []\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59001670",
   "metadata": {},
   "source": [
    "2. In the output terminal you’ll see our program counts `\"go\"` and `\"went\"` as different words! Also, what’s up with `\"mani\"` and `\"hardli\"`? A lemmatizer will fix this. Let’s do it. Where `lemmatizer` is defined, replace `None` with `WordNetLemmatizer()`. Where we defined `lemmatized`, replace the empty list with a list comprehension that uses `lemmatizer` to `lemmatize()` each `token` in `tokenized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4fcb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['so', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'i', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'i', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eadd15",
   "metadata": {},
   "source": [
    "3. Why are the lemmatized verbs like `\"went\"` still conjugated? By default `lemmatize()` treats every word as a noun. Give `lemmatize()` a second argument: `get_part_of_speech(token)`. This will tell our lemmatizer what part of speech the word is. Run your code again to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4765eea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_part_of_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m lemmatized \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token, get_part_of_speech(token)) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmed text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed)\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m lemmatized \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token, \u001b[43mget_part_of_speech\u001b[49m(token)) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmed text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_part_of_speech' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ec0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
