{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c684eed",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 1. Introduction\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "Text preprocessing is an approach for cleaning and preparing text data for use in a specific context. Developers use it in almost all natural language processing (NLP) pipelines, including voice recognition software, search engine lookup, and machine learning model training. It is an essential step because text data can vary. From its format (website, text message, voice recognition) to the people who create the text (language, dialect), there are plenty of things that can introduce noise into your data.\n",
    "\n",
    "<br/>The ultimate goal of cleaning and preparing text data is to reduce the text to only the words that you need for your NLP goals.\n",
    "\n",
    "<br/>In this lesson, you will learn strategies for preparing text data. While this list is not exhaustive, we will cover a few common approaches for cleaning and processing text data. They include:\n",
    "- Using Regex & NLTK libraries\n",
    "- Noise Removal – Removing unnecessary characters and formatting\n",
    "- Tokenization – break multi-word strings into smaller components\n",
    "- Normalization – a catch-all term for processing data; this includes stemming and lemmatization\n",
    "\n",
    "<br/>In the gif below, you can see an example of using noise removal, tokenization, and lemmatization to change the string `\"Who was partying?\"` into a list with the words `\"who\"`, `\"be\"`, and `\"party\"`. In this lesson, you will learn how to use built-in and NLTK functions to apply these same text preprocessing approaches to your own strings.\n",
    "<img src=\"Images/text-preprocessing-introduction.gif\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3508a",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 2. Noise Removal\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "Text cleaning is a technique that developers use in a variety of domains. Depending on the goal of your project and where you get your data from, you may want to remove unwanted information, such as:\n",
    "- Punctuation and accents\n",
    "- Special characters\n",
    "- Numeric digits\n",
    "- Leading, ending, and vertical whitespace\n",
    "- HTML formatting\n",
    "\n",
    "<br/>The type of noise that you need to remove from text usually depends on its source. For example, you could access data via the Twitter API, scraping a webpage, or voice recognition software. Fortunately, you can use the `.sub()` method in Python’s regular expression (`re`) library for most of your noise removal needs.\n",
    "\n",
    "<br/>The `.sub()` method has three required arguments:\n",
    "- `pattern` – a regular expression that is searched for in the input string. There must be an `r` preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
    "- `replacement_text` – text that replaces all matches in the input string\n",
    "- `input` – the input string that will be edited by the `.sub()` method\n",
    "\n",
    "<br/>The method returns a string with all instances of the pattern replaced by the replacement_text. Let’s see a few examples of using this method to remove and replace text from a string.\n",
    "\n",
    "<br/>*Example:*\n",
    "1. First, let’s consider how to remove HTML `<p>` tags from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b039c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\" \n",
    "result = re.sub(r'<.?p>', '', text)\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc551a3",
   "metadata": {},
   "source": [
    "Notice, we replace the tags with an empty string `''`. This is a common approach for removing text.\n",
    "\n",
    "2. Next, let’s remove the whitespace from the beginning of the text. The whitespace consists of four spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd7a37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "text = \"    This is a paragraph\" \n",
    "result = re.sub(r'\\s{4}', '', text)\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7a7d9",
   "metadata": {},
   "source": [
    "3. Remove the opening and closing `h1` tags from `headline_one`. Save the value to `headline_no_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47ec6ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n"
     ]
    }
   ],
   "source": [
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "headline_no_tag = re.sub(r'<.?h1>', '', headline_one)\n",
    "print(headline_no_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf5f15",
   "metadata": {},
   "source": [
    "4. We also saved a Tweet to the variable `tweet`. Remove all `@` characters. Save the result to `tweet_no_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95c8902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "tweet_no_at = re.sub(r'@', '', tweet)\n",
    "print(tweet_no_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571fa53",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 3. Tokenization\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "For many natural language processing tasks, we need access to each word in a string. To access each word, we first have to break the text into smaller components. The method for breaking text into smaller components is called *tokenization* and the individual components are called *tokens.* In other words, tokenization is when a string is broken into a list of substrings. \n",
    "\n",
    "<br/>A few common operations that require tokenization include:\n",
    "- Finding how many words or sentences appear in text\n",
    "- Determining how many times a specific word or phrase exists\n",
    "- Accounting for which terms are likely to co-occur\n",
    "\n",
    "<br/>While tokens are usually individual words or terms, they can also be sentences or other size pieces of text.\n",
    "\n",
    "<br/>To tokenize individual words, we can use `nltk`‘s `word_tokenize()` function. The function accepts a string and returns a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc8e3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6068c50",
   "metadata": {},
   "source": [
    "To tokenize at the sentence level, we can use `sent_tokenize()` from the same module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "869c8b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize this sentence.', 'Also, tokenize this sentence.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63cbf7a",
   "metadata": {},
   "source": [
    "*Example:*\n",
    "1. Import the `word_tokenize()` and `sent_tokenize()` functions from Python’s NLTK package. Tokenize `ecg_text` by word and save the result to `tokenized_by_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c7e2bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "print(tokenized_by_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2dd16",
   "metadata": {},
   "source": [
    "2. Tokenize `ecg_text` by sentence and save the result to `tokenized_by_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b54fd929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)\n",
    "print(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f0da3",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 4. Normalization\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "Tokenization and noise removal are staples of almost all text pre-processing pipelines. However, some data may require further processing through text normalization. Text *normalization* is a catch-all term for various text pre-processing tasks. In the next few exercises, we’ll cover a few of them:\n",
    "- Upper or lowercasing\n",
    "- Stopword removal\n",
    "- Stemming – bluntly removing prefixes and suffixes from a word\n",
    "- Lemmatization – replacing a single-word token with its root\n",
    "\n",
    "<br/>The simplest of these approaches is to change the case of a string. We can use Python’s built-in String methods to make a string all uppercase or lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5b9a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase:  THIS HAS A MIX OF CASES\n",
      "Lower case:  this has a mix of cases\n"
     ]
    }
   ],
   "source": [
    "my_string = 'tHiS HaS a MiX oF cAsEs'\n",
    "print(\"Uppercase: \", my_string.upper())\n",
    "print(\"Lower case: \", my_string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bec98",
   "metadata": {},
   "source": [
    "*Example:*\n",
    "1. Make all the characters in `brands` lowercase and save the results to `brands_lower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "931e383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salvation army, ymca, boys & girls club of america\n"
     ]
    }
   ],
   "source": [
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "brands_lower = brands.lower()\n",
    "print(brands_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080a7a7",
   "metadata": {},
   "source": [
    "2. Make all the `letters` in brands uppercase and save the results to `brands_upper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5900c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALVATION ARMY, YMCA, BOYS & GIRLS CLUB OF AMERICA\n"
     ]
    }
   ],
   "source": [
    "brands_upper = brands.upper()\n",
    "print(brands_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4340a1",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 5. Stopword Removal\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "Stopwords are words that we remove during preprocessing when we don’t care about sentence structure. They are usually the most common words in a language and don’t provide any information about the tone of a statement. They include words such as “a”, “an”, and “the”.\n",
    "\n",
    "<br/>NLTK provides a built-in library with these words. You can import them using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a279ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc2b58",
   "metadata": {},
   "source": [
    "We create a set with the stop words so we can check if the words are in a list below.\n",
    "\n",
    "<br/>Now that we have the words saved to `stop_words`, we can use tokenization and a list comprehension to remove them from a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f66d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'founded', '1926', 'making', 'oldest', 'major', 'broadcast', 'network', 'USA']\n"
     ]
    }
   ],
   "source": [
    "nbc_statement = \"NBC was founded in 1926 making it the oldest major broadcast network in the USA\"\n",
    "word_tokens = word_tokenize(nbc_statement) # Tokenize nbc_statement \n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words] # Remove stop words\n",
    "print(statement_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b577174",
   "metadata": {},
   "source": [
    "*Example:*\n",
    "1. At the top of your script, import stopwords from NLTK. Save all English stopwords, as a set, to a variable called `stop_words`. Then, tokenize the text in `survey_text` and save the result to `tokenized_survey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14cac1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'YouGov', 'study', 'found', 'that', 'American', \"'s\", 'like', 'Italian', 'food', 'more', 'than', 'any', 'other', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenized_survey = word_tokenize(survey_text)\n",
    "print(tokenized_survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e21ea",
   "metadata": {},
   "source": [
    "2. Remove stop words from `tokenized_survey` and save the result to `text_no_stops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cc1cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]\n",
    "print(text_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0ad23",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 5. Stemming\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "In natural language processing, *stemming* is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits.\n",
    "\n",
    "<br/>NLTK has a built-in stemmer called PorterStemmer. You can use it with a list comprehension to stem each word in a tokenized list of words.\n",
    "\n",
    "<br/>First, you must import and initialize the stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae32d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e96c1c",
   "metadata": {},
   "source": [
    "Now that we have our stemmer, we can apply it to each word in a list using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bc8c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e739fd1",
   "metadata": {},
   "source": [
    "Notice, the words like ‘was’ and ‘founded’ became ‘wa’ and ‘found’, respectively. The fact that these words have been reduced is useful for many language processing applications. However, you need to be careful when stemming strings, because words can often be converted to something unrecognizable.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. At the top of your script, import `PorterStemmer`, then initialize an instance of it and save the object to a variable called `stemmer`. Then, tokenize `populated_island` and save the result to `island_tokenized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31d7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "stemmer = PorterStemmer()\n",
    "island_tokenized = word_tokenize(populated_island)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc170b",
   "metadata": {},
   "source": [
    "2. Use a list comprehension to stem each word in `island_tokenized`. Save the result to a variable called `stemmed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb7b4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'it', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28d507",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 6. Lemmatization\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "*Lemmatization* is a method for casting words to their root forms. This is a more involved process than stemming, because it requires the method to know the part of speech for each word. Since lemmatization requires the part of speech, it is a less efficient approach than stemming.\n",
    "\n",
    "<br/>In the next exercise, we will consider how to tag each word with a part of speech. In the meantime, let’s see how to use NLTK’s lemmatize operation.\n",
    "\n",
    "<br/>We can use NLTK’s `WordNetLemmatizer` to lemmatize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03ed2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038c277",
   "metadata": {},
   "source": [
    "Once we have the `lemmatizer` initialized, we can use a list comprehension to apply the lemmatize operation to each word in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6dd4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBC', 'wa', 'founded', 'in', '1926']\n"
     ]
    }
   ],
   "source": [
    "tokenized = [\"NBC\", \"was\", \"founded\", \"in\", \"1926\"]\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5daf3f",
   "metadata": {},
   "source": [
    "The result saved to `lemmatized` contains `'wa'`, while the rest of the words remain the same. Not too useful. This happened because `lemmatize()` treats every word as a noun. To take advantage of the power of lemmatization, we need to tag each word in our text with the most likely part of speech. We’ll do that in the next exercise.\n",
    "\n",
    "<br/>*Example:*\n",
    "1. At the top of the script, import `WordNetLemmatizer`, then initialize an instance of it and save the result to `lemmatizer`. Then, tokenize the string saved to `populated_island`. Save the result to `tokenized_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd5b0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indonesia', 'was', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "print(tokenized_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd380a",
   "metadata": {},
   "source": [
    "2. Use a list comprehension to lemmatize every word in `tokenized_string`. Save the result to the variable `lemmatized_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5fa3d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokenized_string]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc3b74",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 7. Part-of-Speech Tagging\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "To improve the performance of lemmatization, we need to find the part of speech for each word in our string. In the script below, we created a part-of-speech tagging function. The function accepts a word, then returns the most common part of speech for that word. Let’s break down the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62fd005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos()==\"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos()==\"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos()==\"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos()==\"r\"])\n",
    "\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6712be",
   "metadata": {},
   "source": [
    "<br/>*A. Import wordnet and Counter:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f328ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf86509",
   "metadata": {},
   "source": [
    "- `wordnet` is a database that we use for contextualizing words\n",
    "- `Counter` is a container that stores elements as dictionary keys\n",
    "\n",
    "<br/>*B. Get synonyms:*\n",
    "<br/>Inside of our function, we use the `wordnet.synsets()` function to get a set of synonyms for the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24d0aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee8cc1",
   "metadata": {},
   "source": [
    "The returned synonyms come with their part of speech.\n",
    "\n",
    "<br/>*C. Use synonyms to determine the most likely part of speech:*\n",
    "<br/>Next, we create a `Counter()` object and set each value to the count of the number of synonyms that fall into each part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45aaf931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos()==\"n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a4ae7",
   "metadata": {},
   "source": [
    "This line counts the number of nouns in the synonym set.\n",
    "\n",
    "<br/>*D. Return the most common part of speech:*\n",
    "<br/>Now that we have a count for each part of speech, we can use the `.most_common()` counter method to find and return the most likely part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a80778cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_likely_part_of_speech = pos_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614e46c",
   "metadata": {},
   "source": [
    "Now that we can find the most probable part of speech for a given word, we can pass this into our lemmatizer when we find the root for each word. Let’s take a look at how we would do this for a tokenized string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20ff68c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'old', 'be', 'the', 'country', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "tokenized = [\"How\", \"old\", \"is\", \"the\", \"country\", \"Indonesia\"]\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e36685",
   "metadata": {},
   "source": [
    "*Example:*\n",
    "<br/>Use `get_part_of_speech()` to improve your lemmatizer. Under the line where `tokenized_string` is defined, use the `get_part_of_speech()` function in a list comprehension to lemmatize all the words in `tokenized_string`. Save the result to a new variable called `lemmatized_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4798cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "lemmatized_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]\n",
    "print(lemmatized_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a08e2",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 8. Review\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "Congratulations! The goal of this unit was to introduce regular expressions (regex) and several methods to prepare your text data for most NLP tasks.\n",
    "\n",
    "<br/>This lesson is not an exhaustive introduction to text preprocessing. However, it does show a few of the most common tricks for cleaning your data. Before building a text preprocessing pipeline, it’s most important to have an idea of how you want your data formatted and why you want it formatted that way. Once you know what you want, you can use these tools to get you there.\n",
    "\n",
    "<br/>Having completed this unit, you are now able to:\n",
    "- Implement basic regular expressions.\n",
    "- Recognize several techniques to prepare text for various NLP tasks.\n",
    "- Use Python and regex to remove unnecessary formatting from your text.\n",
    "- Split text into tokens using NLTK.\n",
    "- Normalize text with Python, regex, and NLTK by removing affixes, changing case, and removing common words.\n",
    "\n",
    "<br/>Let’s review what we covered in this lesson:\n",
    "- Text preprocessing is all about cleaning and prepping text data so that it’s ready for other NLP tasks.\n",
    "- Noise removal is a text preprocessing step concerned with removing unnecessary formatting from our text.\n",
    "- Tokenization is a text preprocessing step devoted to breaking up text into smaller units (usually words or discrete terms).\n",
    "- Normalization is the name we give most other text preprocessing tasks, including stemming, lemmatization, upper and lowercasing, and stopword removal.\n",
    "- Stemming is the normalization preprocessing task focused on removing word affixes.\n",
    "- Lemmatization is the normalization preprocessing task that more carefully brings words down to their root forms.\n",
    "\n",
    "<br/>If you are interested in learning more about these topics, here are some additional resources:\n",
    "- Book: [Speech and Language Processing, Chapter 2, Daniel Jurafsky & James H. Martin](https://web.stanford.edu/~jurafsky/slp3/2.pdf)\n",
    "- Video Playlist: [NLTK with Python 3 for Natural Language Processing](https://www.youtube.com/playlist?reload=9&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL)\n",
    "\n",
    "<br/>*Example:*\n",
    "<br/>Below, is random HTML text. See if you can use only the skills you’ve learned in this lesson to:\n",
    "- Select only the string within the `<p>` tags.\n",
    "- Remove all periods.\n",
    "- Make all of the words lowercase.\n",
    "- Tokenize the string.\n",
    "- Lemmatize the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc3ee19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'in', 'local', 'medium', ',', 'she', 'be', 'both', 'the', 'young', 'news', 'anchor', 'and', 'the', 'first', 'black', 'female', 'news', 'anchor', 'at', 'nashville', \"'s\", 'wlac-tv']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "oprah_wiki = '<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville\\'s WLAC-TV. </p>'\n",
    "\n",
    "oprah_no_html = re.sub(r'<.?p>', '', oprah_wiki)\n",
    "oprah_text = re.sub(r'\\.', '', oprah_no_html)\n",
    "oprah_text = oprah_text.lower()\n",
    "oprah_tokenized = word_tokenize(oprah_text)\n",
    "oprah_lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in oprah_tokenized]\n",
    "\n",
    "print(oprah_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66488b95",
   "metadata": {},
   "source": [
    "<img src=\"Images\\atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 9. NLTK with Python 3 for Natural Language Processing\n",
    "*Text Preprocessing*\n",
    "\n",
    "----\n",
    "We recommend watching the following videos for helpful tutorials on using NLTK for text preprocessing:\n",
    "\n",
    "<br/>[Natural Language Processing With Python and NLTK p.1 Tokenizing words and Sentences](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)\n",
    "\n",
    "<br/>In this video, you will get a tutorial on tokenizing text using NLTK. This is helpful if you want to see a demo of how to break text data up into words or sentences.\n",
    "\n",
    "<br/>[Stop Words - Natural Language Processing With Python and NLTK p.2](https://pythonprogramming.net/stop-words-nltk-tutorial/)\n",
    "\n",
    "<br/>In this video, you will get a tutorial on removing stopwords from text data using NLTK, which is helpful if you’re preparing text for sentiment analysis, topic modeling, or other tasks where common words are not helpful.\n",
    "\n",
    "<br/>[Stemming - Natural Language Processing With Python and NLTK p.3](https://pythonprogramming.net/stemming-nltk-tutorial/)\n",
    "\n",
    "<br/>[Lemmatizing - Natural Language Processing With Python and NLTK p.8](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)\n",
    "\n",
    "<br/>In these videos, you will get tutorials on stemming and lemmatization using NLTK, further explaining the differences between these two techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
